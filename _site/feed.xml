<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mk's</title>
    <description>Junior Software Engineer
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 07 Apr 2021 21:14:09 +0900</pubDate>
    <lastBuildDate>Wed, 07 Apr 2021 21:14:09 +0900</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>칼만 필터는 어렵지 않아</title>
        <description>&lt;h1 id=&quot;칼만필터는-어렵지않아&quot;&gt;칼만필터는 어렵지않아&lt;/h1&gt;

&lt;h2 id=&quot;평균-필터&quot;&gt;평균 필터&lt;/h2&gt;

&lt;p&gt;배치식 : 데이터를 한번에 모아 계산&lt;/p&gt;

&lt;p&gt;⇒ 아주 멍청한 방식 , 이런식의 필터는 패기처분&lt;/p&gt;

&lt;p&gt;재귀식 : 이전 결과값을 이용하여 계산 &lt;strong&gt;(재귀식과 재귀함수는 다르다)
⇒ 계산 효율이 배치식보다 좋음 ,  메모리 공간 효율도 좋음&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled.png&quot; alt=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled%201.png&quot; alt=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 때 a는 데이터 개수 k 에 의해 정해진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;평균계산 , 센서 초기화(영점 조절)에 사용됨 (평균 필터를 통해 측정 데이터의 잡음을 제거 할 수 있다)
⇒ &lt;strong&gt;동적 시스템 (시간에 따라 변화)은 반영할 수 없음&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;AvgFilter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_avg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#데이터수,값,평균
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;이동평균-필터&quot;&gt;이동평균 필터&lt;/h2&gt;

&lt;p&gt;모든 데이터의 평균이 아닌 오래된 측정값을 버리는 평균 필터&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;측정한 물리량이 시간에 따라 변하는 경우 (동적 시스템) 잡음을 없애는 방법 중 하나&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;⇒ &lt;strong&gt;평균 필터와는 다르게 n개의 최근 데이터를 저장해야 하므로 메모리 저장공간이 절약되는 이점은 없음&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 개수(n)가 많으면 시간 지연이 발생,  잡음 제거 효과 상승&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled%202.png&quot; alt=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;MovAvgFilter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;mvag&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mea&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mvag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_val&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;저주파-통과-필터-low-pass-filter&quot;&gt;저주파 통과 필터 (Low pass filter)&lt;/h2&gt;

&lt;p&gt;저주파만 통과 시키는 필터 ( 잡음은 고주파이므로 잡음 걸러내는데 쓰임)&lt;/p&gt;

&lt;p&gt;⇒ 영상처리에서도 배웠다 싶이 종류가 매우 많음 이 섹션에선 &lt;strong&gt;1차 저주파 통과 필터( 지수 가중 이동평균 필터)&lt;/strong&gt; 를 다룸&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이동 평균 필터는 모든 데이터에 같은 가중치(1/n)를 부여 하므로 성능이 좋지 못함 (즉, 오래된 데이터와 최근의 데이터의 가중치가 동일)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled%203.png&quot; alt=&quot;%E1%84%8F%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A1%E1%86%AB%E1%84%91%E1%85%B5%E1%86%AF%E1%84%90%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%A5%E1%84%85%E1%85%A7%E1%86%B8%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AD%E1%84%8B%E1%85%A1%20a76be9a2f30c48c9bf6b6bb11d5f8886/Untitled%203.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평균필터와 다르게 a는 임의로 지정 한 값&lt;/p&gt;

&lt;p&gt;⇒ 점화식을 풀어보면 이전값 일수록 1보다작은 a값이 여러번 곱해지므로 오래된 값의 가중치는 작아진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;측정 데이터의 상황에 맞게 a를 선정해야함 a를 잘못 선정하게 되면 이동평균필터보다 시간지연이 더 많이 생길수도 있음.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;a가 크면 잡음은 줄어들고 시간지연이 커짐&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;LowPassFilter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;est&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;칼만-필터&quot;&gt;칼만 필터&lt;/h2&gt;
</description>
        <pubDate>Tue, 06 Apr 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/cs/2021/04/06/CS-test/</link>
        <guid isPermaLink="true">http://localhost:4000/cs/2021/04/06/CS-test/</guid>
        
        <category>algo2</category>
        
        
        <category>CS</category>
        
      </item>
    
      <item>
        <title>ML lecture summary</title>
        <description>&lt;h1 id=&quot;예제&quot;&gt;예제&lt;/h1&gt;

&lt;h2 id=&quot;machine-learning-basics&quot;&gt;Machine Learning Basics&lt;/h2&gt;

&lt;h3 id=&quot;ml-overview&quot;&gt;ML Overview&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised Learning  : 정답(label)을 제공하면서 학습 시키고 문제를 해결하는 방법을 제공하면서 학습 시키고 문제를 해결하는 방법
⇒ 본 강의는 주로 이내용을 다룸&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Regression : 데이터의 추정치를 근사하는 식을 만드는 것
  ⇒&lt;/strong&gt; x,y좌표를 받아 차량의 heading값을 구할 수 있는 프로젝트를 해볼까? &lt;em&gt;**&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Classification : 데이터를 구분하는 식을 만드는 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning  :  정답(label)을 제공하지 않고 경향성을 찾도록 하는 방법&lt;/li&gt;
  &lt;li&gt;Reinforcement Learning : supervised와 Unsupervised를 환경에 따라 적절하게 사용하느것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Machine Learning : 사람이 관여한 학습&lt;/p&gt;

&lt;p&gt;Deep Learning : 전면 자동 학습&lt;/p&gt;

&lt;p&gt;Model Parameters : 학습결과&lt;/p&gt;

&lt;p&gt;HyperParameters : 유저가 설정한 값 ex)모델 사이즈&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Split for ML : 처음에 data를 train data와 test data를 나누고 나서
train data 또한 모의고사용으로 validation을 따로 주어야 테스트전에 평향되지 않은 결과로 검증된 test할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h3&gt;

&lt;p&gt;정의역  x를 입력으로 주면 y를 알 수 있음&lt;/p&gt;

&lt;p&gt;⇒ 확장한 &lt;strong&gt;Linear Prediction ( 여러차원의 input )&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/test/mltest/1.png&quot; alt=&quot;1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/test/mltest/2.png&quot; alt=&quot;2.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loss function(손실 함수) : 예상값과 실제값과의 차이를 함수로 정의 한 것&lt;/li&gt;
  &lt;li&gt;Optimization : 손실 함수를 줄일 수 있게 Model parameters에  피드백을 주는 행동&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nearest-neighbors&quot;&gt;Nearest Neighbors&lt;/h3&gt;

&lt;p&gt;⇒ test시 가장 거리가 가까운 label의 class로 판단&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Image Classification : 이미지를 주면 이게 무엇인지 보는 것&lt;/p&gt;

    &lt;p&gt;⇒Semantic Gap,viewpoint variation, Background Clutter, illumination. Occlusion. Intar-class variation, Inter-class variation 등의 이슈가 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Intra-class variation VS Inter-class variation&lt;/p&gt;

    &lt;p&gt;클래스 내의 분포 VS 클래스 간의 거리 
  ⇒작을 수록               ⇒ 멀수록                좋다.  (구분감)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Distance Metric : 거리계산 공식 ex) L1 , L2&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;⇒ L1은 Mangattan 거리로 x와 y의 합을 구한 거리이고
     L2는 Euclidean 거리로 x와 y따로 구하지않고 제곱한 거리이다.(선호)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Nearest Neighbors 특징&lt;/p&gt;

    &lt;p&gt;모든 data를 기억하고 test시 비교해서 결과를 내므로 training은 빠를 수 있으나 기억해야하는 data도 많고 test시 시간이 오래걸린다.
  또한 어떠한 사람이 봤을때 항상 구분이 가능하나 이것은 구분을 못할 수 있다. (인위적으로 한부분 가린경우)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dimensionality-reductionexpansion&quot;&gt;Dimensionality Reduction/Expansion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Dimension Expansion
⇒ 클래스 내의 분포와 거리가 애매해서  구분이 어려운 경우 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%202.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dimension을 확장하여 classifier performance를 증가 시킬 수 있으나, 과하게 확장시 감소한다. 즉, 적당한 Dimenstion Expansion을 해야함.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dimension Reduction&lt;/p&gt;

    &lt;p&gt;PCA : 최적의 표현을 할 수 있는 최대 분산을 찾도록 차원을 축소하는것&lt;/p&gt;

    &lt;p&gt;LDA : 클래스의 구분감을 올리기 위해 차원을 축소하는것&lt;/p&gt;

    &lt;p&gt;t-sne: 시각화를 위해 차원을 축소하는것&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-classifier&quot;&gt;Linear Classifier&lt;/h3&gt;

&lt;p&gt;: W(weights)를 조정하여 label class의 score를 증가시켜 Classifier를 수행함&lt;/p&gt;

&lt;p&gt;input x→ f(x,W) → Class scores
이 때 Class scores 들을 이용하여 정답 scores와 비교를 통한 다양한 Loss Function을 줄여 Weight를 조정한다. (Optimization)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;nonlinear한 경우 Layer 사이에 Nonlinearity activation function을 더해서 해결한다. ( 이것은 뒤에 Deep Neural Network의 기본이 됨)
⇒ W1*W2 (선형적인) 해도 Linear이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;svm-classifier-vs-softmax-classifer&quot;&gt;SVM classifier vs Softmax Classifer&lt;/h3&gt;

&lt;p&gt;⇒ Loss function (손실함수) 비교&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SVM Loss function(hinge loss)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%203.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%203.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;⇒margin 값만 넘기면 학습 종료하므로 상대적으로 욕심이 적다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Softmax Loss function(Cross-Entropy Loss)
⇒확률이 합이 1이라는 성질을 다중 분류에 사용 (Multi-class classification에 주로 사용됨)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%204.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%204.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;정확한 예측을 할 수록 L의 크기는 작아진다. 그러나 예측을 잘 못할수록 L이 커짐  이를 이용하여 loss함수를 최소화 시키는 방향으로 weight를 업데이트&lt;/p&gt;

    &lt;p&gt;⇒ corrct class score가 1.00이 되어야 학습을 종료하므로 상대적으로 욕심이 많다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(추가)  MSE(mean squared error) :
regression(회귀) 용도의 딥러닝 모델을 훈련할 때 사용되는 손실 함수&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%205.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%205.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Object Detector인 Yolov4 모델의 Loss function인 &lt;strong&gt;Focal loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%206.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%206.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;⇒ 예측을 잘 한 class에 대해서는 loss를 적게 해 loss갱신을 거의 못하게하고,  예측을 제대로 하지 못한 class에 대해서는 loss를 크게 줌&lt;/p&gt;

&lt;h2 id=&quot;neural-networks-1&quot;&gt;Neural Networks 1&lt;/h2&gt;

&lt;h3 id=&quot;introduction-to-neural-networks&quot;&gt;Introduction to Neural Networks&lt;/h3&gt;

&lt;p&gt;인간의 Neural을 모방한 Networks&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;n-layer-Neural Network 
linear score function에서 확장하여 Activation function (ex &lt;strong&gt;ReLu&lt;/strong&gt;(max(0,x))) 를 이용하여 구성됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%207.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%207.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ReLu(활성함수)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%208.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%208.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h3&gt;

&lt;p&gt;⇒ 최선의 W를 구하는 방법이 무엇일까? ( 랜덤하게 업데이트하게되도 생각보다 괜찮은 결과가 나온다 그러나 &lt;strong&gt;기울기&lt;/strong&gt;를 따라 진행해보면 어떨까? 에서시작 )&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%209.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%209.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;gradient을 통해서 변화량을 구하고 0이 되는 방향으로 W를 조정&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Analytic Gradient : 편미분을 통하여 빠르고 정확하게 계산&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-graph--backpropagation&quot;&gt;Computational Graph &amp;amp; Backpropagation&lt;/h3&gt;

&lt;p&gt;⇒ 계산이 복잡해지면 Loss에 대해서 gradient를 구하기 어려울 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computational Graph : 연산을 그래프화&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Backpropagation(역전파) : 미분을 통해 맨 뒤에서 부터 거꾸로 전파하여 W와 Bias를 조정&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2010.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2010.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2011.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2011.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2012.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2012.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regularization loss / R(W) : overfiting을 예방하기 위한 것 ( &lt;strong&gt;test에 맞추기 위한건데 너무 train에 맞춰지면 안됨&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;Loss는 앞서구한 data loss와 Regularization loss를 더해서 구한다. ⇒ L2를 대부분 사용&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2013.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2013.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;neural-networks-2&quot;&gt;Neural Networks 2&lt;/h2&gt;

&lt;h3 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h3&gt;

&lt;p&gt;Optimization 를 효율적으로 하기위해 전처리 하는과정&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;zero-centered data : 영점조절 (기본)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;normalized data : 축 scale조절&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;decorrelated data : 각 축에 직교하게 만듬 ( correlation을 없앰)&lt;/li&gt;
  &lt;li&gt;whitened data : 축에 대해서 분산량이 동일하게 원의 형태로 만듬&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;⇒PCA 와 wthiening은 deep learning  에서 잘안쓴다&lt;/p&gt;

&lt;h3 id=&quot;weight-initialization&quot;&gt;Weight Initialization&lt;/h3&gt;

&lt;p&gt;NN 요약 :  input layer→ Hidden layer (W) →Ouput layer : 사이사이는 nonlinear이지만 마지막은 linear!&lt;/p&gt;

&lt;p&gt;( Not guaranteed to reach the optimum, 기울기가 0이라고 optimum이라고 하면안됨)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Global Optimum: 결론적으로 원하는 값&lt;/li&gt;
  &lt;li&gt;Local Minimum : non-convex할때 생긴 지점&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;W가 작은값으로 초기값을 갖게 될때 문제가 생김 (Activation function을 tanh로 사용할 때 )
⇒ Layer를 통과시킬수록 가운데에 몰리기때문에 gradient도 작아지므로 업데이트가 발생하지 않음&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2014.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2014.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2015.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2015.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;W가 상대적으로 큰경우 (tanh) ⇒ 마찬가지로 기울기가 0 이되므로 문제가 생김 ( 사이드로 과포화 )&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Xavier Initialization : 위의 문제를 해결하는 방법 표준 정규 분포를 입력 차원의 수로 스케일링 
⇒ ReLu 사용시 0신호가 가까워 지므로 2를 추가로 곱해주는 Kalming / MSRA Initialization 함Xavier Initialization : 위의 문제를 해결하는 방법 표준 정규 분포를 입력 차원의 수로 스케일링 ⇒ ReLu 사용시 0신호가 가까워 지므로 2를 추가로 곱해주는 Kalming / MSRA Initialization 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stocahstic-gradient-descent&quot;&gt;Stocahstic Gradient Descent&lt;/h3&gt;

&lt;p&gt;데이터를 랜덤하게 샘플링 → traing → 다시 랜덤하게 샘플링 → traing → 반복&lt;/p&gt;

&lt;p&gt;(통계적인 측면에서 좋은 성질을 이용)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch : 수행 할 수 있는 범위 내에서 분활한 subset&lt;/li&gt;
  &lt;li&gt;Epoch : 하나의 전체 데이터셋을 한바퀴 도는 것의 단위&lt;/li&gt;
  &lt;li&gt;Iteration : Epoch을 도는데 필요한 Batch 수&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fancier-optimizers&quot;&gt;(Fancier) Optimizers&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2016.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2016.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;⇒ SGD도 Optimizers의 하나의 예지만, 문제가있다. 시간이 오래걸리거나 , Local minimum에 갖힐 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SGD + Momentum : velocity도 고려하여 gradient와 velocity의 벡터 합을 통해 실제 변화량을 계산하여 gradient이 0인 점에 갖히지 않도록함&lt;/li&gt;
  &lt;li&gt;AdaGrad : gradient가 급격한경우 완만하게, 완만한경우 급격하게 변화 (제곱 이용)&lt;/li&gt;
  &lt;li&gt;Adam : Momentum + AdaGrad (보통 이걸써라, SGD는 튜닝하기 어려울수 있음)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;regularization-1&quot;&gt;Regularization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Overfitting : 학습할수록 train Accuract는 올라가나 val Accuract는 잘 올라가지 않을 때
⇒&lt;strong&gt;더 많은 data 수집, regularization 규제, dropout 등과 같은 방법으로 Overfitting을 막을 수 잇음&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Underfitting : 학습후 train과 val의 Accuract차이는 크지않으나 실제test에서 낮은 Accuract를 나타낼 때
&lt;strong&gt;⇒epochs횟수 늘리기, parameter가 더 많은 복잡한 모델 선택,  model의 규제를 줄이는 방법 등으로 막을 수 있음&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2017.png&quot; alt=&quot;%E1%84%8B%E1%85%A8%E1%84%8C%E1%85%A6%203d3f8f09c229456399df11716fd7d417/Untitled%2017.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Ensembles : 과대적합 시 사용하는 트릭 중 하나로 학습시 모델을 살짝 변화시킨 여러 모델들을 학습시키고 결과 확률의 평균중 최대값을 사용한다. &lt;strong&gt;너무많은 학습량을 필요로 하므로 개인 레벨에서는 부적합하다.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Regularization loss (Weight decay)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropout : 일정 확률로 노드 연결을 끊어 feature간의 연결성을 낮추고(co-adaptation) 분별력을 높임, Ensembles의 효과를 낼 수 있음&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Dropout의 test time에는&lt;/strong&gt; 모든 노드를 키고 score값에 노드의 확률을 곱하여 계산함&lt;/p&gt;

    &lt;p&gt;⇒ 학습시에는 p른 나눠줌 ( Inverted dropout )&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Data Augmentation : data를 실제 상황에서 일어날 수 있을 법한 변형을 적용
ex) Horizontal Flips (좌우대칭) , 상하대칭은 사용X(실제 잘 안일어남)
Random crops and scales (랜덤하게 patch를 잘라냄)
Color Jitter (픽셀 밝기나 contrast를 조절) …&lt;/li&gt;
  &lt;li&gt;DropConnect : 노드는 살리고 Weight를 끊는 방법&lt;/li&gt;
  &lt;li&gt;CutOut : 이미지 내에서 ramdom영역을 지워 학습&lt;/li&gt;
  &lt;li&gt;Mixup : 랜덤으로 다른 두 class를 blend한 이미지를 목표label score을 blend 비율로 나누어서 학습&lt;/li&gt;
  &lt;li&gt;Cutmix : CutOut+Mixup&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;⇒ 여러가지 짬봉하는 기법이나 실제 있을법한 환경으로 학습을 다양하게 해보자&lt;/p&gt;

&lt;h3 id=&quot;hyperparameter-tuning&quot;&gt;Hyperparameter Tuning&lt;/h3&gt;

&lt;p&gt;: 성능을 최적화하거나, bias과 variance사이의 균형을 맞출 때, 알고리즘을 조절하기 위해 사용.  학습 전에 미리 지정되어 훈련하는 동안은 상수로 남게 되며, 가중치와 같은 파라미터와 다르게 주로 알고리즘 사용자에 의해 정해짐
→ 과소적합, 과대적합을 피하기 위해 적당한 값이 설정&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hyperparameter searching 
Random Search VS Grid Search : Ramdom이 최적 구간을 잘 잡을 수 있음 주로 사용Coarse to Fine : Random Search를 할때 모든 구간을 Search하지 않고 lough하게 진행하고 결과가 좋은 일정 구간을 search함&lt;/li&gt;
  &lt;li&gt;Choosing Hyperparameters Guidelines
&lt;strong&gt;step1. Check inital loss&lt;/strong&gt; : 초기에 과대적합을 고민할 필요 없으므로 regulaization loss(weight decay)를 끄고 data loss만 사용
&lt;strong&gt;step2. Overfit a small sample :&lt;/strong&gt; 학습데이터에 대해서 조금만 샘플링 후 경향성을 분석
&lt;strong&gt;step3. Find Learning Rate that makes loss go down&lt;/strong&gt; : 전체 data에 대해 loss가 감소하는 learning rate 찾고 난 후 regulaization loss 고려함
&lt;strong&gt;step4. Coarse grid, train for ~1-5 epochs&lt;/strong&gt; : coarse하게 몇개의 value를 찾고
&lt;strong&gt;step5. Refine grid, tarin longer&lt;/strong&gt; :  확인 후 가장 성능 좋은 모델의 하이퍼파라미터로 부터 grid를 refine. 
&lt;strong&gt;step6. Look at loss curves&lt;/strong&gt; : loss와  Accuracy를 분석 (경향성 분석)
&lt;strong&gt;step7. setp4로 돌아가 반복&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 05 Apr 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/test/2021/04/05/test-test1/</link>
        <guid isPermaLink="true">http://localhost:4000/test/2021/04/05/test-test1/</guid>
        
        <category>review</category>
        
        <category>ML</category>
        
        
        <category>Test</category>
        
      </item>
    
  </channel>
</rss>
